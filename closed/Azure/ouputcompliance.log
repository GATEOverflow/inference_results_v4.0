make[1]: Entering directory '/work'
TEST01 trial 1
make[2]: Entering directory '/work'
[2024-02-28 16:42:51,126 systems.py:197 INFO] Found unknown device in GPU connection topology: NIC0. Skipping.
[2024-02-28 16:42:51,180 main.py:229 INFO] Detected system ID: KnownSystem.NC_H100_v5
[2024-02-28 16:42:51,257 run_audit.py:98 INFO] => Compliance test TEST01 is exempted for llama2-70b-Offline. You can safely ignore this message.
[2024-02-28 16:42:51,257 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:42:51,257 run_audit.py:320 INFO] => Submission checker: Audit test TEST01 PASS (by default, due to exemption)
[2024-02-28 16:42:51,266 run_audit.py:98 INFO] => Compliance test TEST01 is exempted for llama2-70b-Offline. You can safely ignore this message.
[2024-02-28 16:42:51,266 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:42:51,266 run_audit.py:320 INFO] => Submission checker: Audit test TEST01 PASS (by default, due to exemption)
[2024-02-28 16:42:51,275 run_audit.py:98 INFO] => Compliance test TEST01 is exempted for llama2-70b-Server. You can safely ignore this message.
[2024-02-28 16:42:51,275 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:42:51,275 run_audit.py:320 INFO] => Submission checker: Audit test TEST01 PASS (by default, due to exemption)
[2024-02-28 16:42:51,283 run_audit.py:98 INFO] => Compliance test TEST01 is exempted for llama2-70b-Server. You can safely ignore this message.
[2024-02-28 16:42:51,283 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:42:51,283 run_audit.py:320 INFO] => Submission checker: Audit test TEST01 PASS (by default, due to exemption)
make[2]: Leaving directory '/work'
make[1]: Leaving directory '/work'
make[1]: Entering directory '/work'
TEST04 trial 1
make[2]: Entering directory '/work'
Sleep to reset thermal state before TEST04...
[2024-02-28 16:43:16,223 systems.py:197 INFO] Found unknown device in GPU connection topology: NIC0. Skipping.
[2024-02-28 16:43:16,275 main.py:229 INFO] Detected system ID: KnownSystem.NC_H100_v5
[2024-02-28 16:43:16,353 run_audit.py:98 INFO] => Compliance test TEST04 is exempted for llama2-70b-Offline. You can safely ignore this message.
[2024-02-28 16:43:16,353 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:43:16,353 run_audit.py:320 INFO] => Submission checker: Audit test TEST04 PASS (by default, due to exemption)
[2024-02-28 16:43:16,362 run_audit.py:98 INFO] => Compliance test TEST04 is exempted for llama2-70b-Offline. You can safely ignore this message.
[2024-02-28 16:43:16,362 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:43:16,362 run_audit.py:320 INFO] => Submission checker: Audit test TEST04 PASS (by default, due to exemption)
[2024-02-28 16:43:16,371 run_audit.py:98 INFO] => Compliance test TEST04 is exempted for llama2-70b-Server. You can safely ignore this message.
[2024-02-28 16:43:16,371 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:43:16,371 run_audit.py:320 INFO] => Submission checker: Audit test TEST04 PASS (by default, due to exemption)
[2024-02-28 16:43:16,379 run_audit.py:98 INFO] => Compliance test TEST04 is exempted for llama2-70b-Server. You can safely ignore this message.
[2024-02-28 16:43:16,379 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:43:16,379 run_audit.py:320 INFO] => Submission checker: Audit test TEST04 PASS (by default, due to exemption)
make[2]: Leaving directory '/work'
make[1]: Leaving directory '/work'
make[1]: Entering directory '/work'
TEST05 trial 1
make[2]: Entering directory '/work'
Sleep to reset thermal state before TEST05...
[2024-02-28 16:43:41,326 systems.py:197 INFO] Found unknown device in GPU connection topology: NIC0. Skipping.
[2024-02-28 16:43:41,378 main.py:229 INFO] Detected system ID: KnownSystem.NC_H100_v5
[2024-02-28 16:43:41,457 run_audit.py:98 INFO] => Compliance test TEST05 is exempted for llama2-70b-Offline. You can safely ignore this message.
[2024-02-28 16:43:41,457 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:43:41,457 run_audit.py:320 INFO] => Submission checker: Audit test TEST05 PASS (by default, due to exemption)
[2024-02-28 16:43:41,466 run_audit.py:98 INFO] => Compliance test TEST05 is exempted for llama2-70b-Offline. You can safely ignore this message.
[2024-02-28 16:43:41,466 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:43:41,466 run_audit.py:320 INFO] => Submission checker: Audit test TEST05 PASS (by default, due to exemption)
[2024-02-28 16:43:41,475 run_audit.py:98 INFO] => Compliance test TEST05 is exempted for llama2-70b-Server. You can safely ignore this message.
[2024-02-28 16:43:41,475 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:43:41,475 run_audit.py:320 INFO] => Submission checker: Audit test TEST05 PASS (by default, due to exemption)
[2024-02-28 16:43:41,484 run_audit.py:98 INFO] => Compliance test TEST05 is exempted for llama2-70b-Server. You can safely ignore this message.
[2024-02-28 16:43:41,484 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:43:41,484 run_audit.py:320 INFO] => Submission checker: Audit test TEST05 PASS (by default, due to exemption)
make[2]: Leaving directory '/work'
make[1]: Leaving directory '/work'
make[1]: Entering directory '/work'
[2024-02-28 16:43:44,953 systems.py:197 INFO] Found unknown device in GPU connection topology: NIC0. Skipping.
[2024-02-28 16:43:45,005 main.py:229 INFO] Detected system ID: KnownSystem.NC_H100_v5
[2024-02-28 16:43:45,084 run_audit.py:142 INFO] => Running compliance harness for test TEST06
[2024-02-28 16:43:45,084 auditing.py:139 INFO] AUDIT HARNESS: Looking for audit.config in build/inference/compliance/nvidia/TEST06/llama2-70b/audit.config...
[2024-02-28 16:43:45,084 auditing.py:143 INFO] AUDIT HARNESS: Search failed. Looking for audit.config in build/inference/compliance/nvidia/TEST06/audit.config...
[2024-02-28 16:43:45,087 generate_conf_files.py:107 INFO] Generated measurements/ entries for NC_H100_v5_TRT/llama2-70b-99/Offline
[2024-02-28 16:43:45,088 __init__.py:46 INFO] Running command: mpirun -np 3 ./build/bin/harness_llm --logfile_outdir="build/compliance_logs/TEST06/NC_H100_v5_TRT/llama2-70b-99/Offline" --logfile_prefix="mlperf_log_" --performance_sample_count=24576 --gpu_batch_size=1300 --tensor_path="build/preprocessed_data/open_orca/input_ids_padded.npy,build/preprocessed_data/open_orca/input_lens.npy" --use_graphs=false --gpu_inference_streams=1 --gpu_copy_streams=1 --tensor_parallelism=2 --pipeline_parallelism=1 --kvcache_free_gpu_mem_frac=0.9 --enable_sort=false --llm_gen_config_path="code/llama2-70b/tensorrt/generation_config.json" --use_token_latencies=true --gpu_engines="./build/engines/NC_H100_v5/llama2-70b/Offline/bs1300-custom_k_99_MaxP-tp2-pp1/rank0.engine" --mlperf_conf_path="build/loadgen-configs/NC_H100_v5_TRT/llama2-70b-99/Offline/mlperf.conf" --user_conf_path="build/loadgen-configs/NC_H100_v5_TRT/llama2-70b-99/Offline/user.conf" --scenario Offline --model llama2-70b
[2024-02-28 16:43:45,088 __init__.py:53 INFO] Overriding Environment
benchmark : Benchmark.LLAMA2
buffer_manager_thread_count : 0
coalesced_tensor : True
data_dir : /mnt/resource_nvme/scratch/data
enable_sort : False
gpu_batch_size : 1300
gpu_copy_streams : 1
gpu_inference_streams : 1
input_dtype : int32
input_format : linear
kvcache_free_gpu_mem_frac : 0.9
llm_gen_config_path : code/llama2-70b/tensorrt/generation_config.json
log_dir : build/compliance_logs/TEST06
offline_expected_qps : 13.5
pipeline_parallelism : 1
precision : fp16
preprocessed_data_dir : /mnt/resource_nvme/scratch/preprocessed_data
scenario : Scenario.Offline
system : SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='AMD EPYC 9V84 96-Core Processor', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=80, threads_per_core=1): 1}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=660.463936, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=660463936000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA H100 NVL', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=93.583984375, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=100485038080), max_power_limit=400.0, pci_id='0x232110DE', compute_sm=90): 2})), numa_conf=NUMAConfiguration(numa_nodes={}, num_numa_nodes=2), system_id='NC_H100_v5')
tensor_parallelism : 2
tensor_path : build/preprocessed_data/open_orca/input_ids_padded.npy,build/preprocessed_data/open_orca/input_lens.npy
use_fp8 : True
use_graphs : False
use_token_latencies : True
system_id : NC_H100_v5
config_name : NC_H100_v5_llama2-70b_Offline
workload_setting : WorkloadSetting(HarnessType.Custom, AccuracyTarget.k_99, PowerSetting.MaxP)
optimization_level : plugin-enabled
num_profiles : 1
config_ver : custom_k_99_MaxP
accuracy_level : 99%
inference_server : custom
audit_test_name : TEST06
skip_file_checks : False
power_limit : None
cpu_freq : None
&&&& RUNNING LLM_HARNESS # ./build/bin/harness_llm
[I] Initializing TensorRT plugin library...
[I] Initializing TRT-LLM plugin libraries...
&&&& RUNNING LLM_HARNESS # ./build/bin/harness_llm
[I] Initializing TensorRT plugin library...
[I] Initializing TRT-LLM plugin libraries...
&&&& RUNNING LLM_HARNESS # ./build/bin/harness_llm
[I] Initializing TensorRT plugin library...
[I] Initializing TRT-LLM plugin libraries...
I0228 16:43:48.580823 21351 main_llm.cc:170] Loading LLM config from "./build/engines/NC_H100_v5/llama2-70b/Offline/bs1300-custom_k_99_MaxP-tp2-pp1/config.json" and geneartion config from: code/llama2-70b/tensorrt/generation_config.json
I0228 16:43:48.605573 21352 main_llm.cc:170] Loading LLM config from "./build/engines/NC_H100_v5/llama2-70b/Offline/bs1300-custom_k_99_MaxP-tp2-pp1/config.json" and geneartion config from: code/llama2-70b/tensorrt/generation_config.json
I0228 16:43:48.607342 21350 main_llm.cc:170] Loading LLM config from "./build/engines/NC_H100_v5/llama2-70b/Offline/bs1300-custom_k_99_MaxP-tp2-pp1/config.json" and geneartion config from: code/llama2-70b/tensorrt/generation_config.json
I0228 16:43:48.843817 21351 main_llm.cc:284] Rank: 1 has pid: 21351
I0228 16:43:48.843844 21351 main_llm.cc:121] Found 2 GPUs
I0228 16:43:48.843885 21352 main_llm.cc:284] Rank: 2 has pid: 21352
I0228 16:43:48.843883 21350 main_llm.cc:281] LLMConfig Details:
	mMaxSumSeqlen: 2048 mMaxInputSeqlen: 1024 mMaxOutputSeqlen: 1024 mEosId: 2
	tp: 2 pp: 1 mWorldSize: 2
	mMaxGpuBatchSize: 1300 mBeamWidth: 1
	mTemperature: 1 mTopK: 1 mTopP: 0 mMinOutputSeqlen: 1 mIsStreaming: 0 mReportFirstToken: 0 mEnableSort: 0 mExcludeInputInOutput: 1
	mMaxNumSequences: 0 mMaxTokensInPagedKvcache: 0 mKvCacheFreeGpuMemFraction: 0.9 mEnableTrtOverlap: 0
	mBatchMode: 2 mSchedulerPolicy: 0
I0228 16:43:48.843911 21350 main_llm.cc:284] Rank: 0 has pid: 21350
I0228 16:43:48.843916 21350 main_llm.cc:121] Found 2 GPUs
I0228 16:43:48.843904 21352 main_llm.cc:121] Found 2 GPUs
I0228 16:43:48.844092 21350 main_llm.cc:506] Rank 0:: local session [rank/size | color | devId]: [0/2 | 0 | 0]
I0228 16:43:48.844106 21350 main_llm.cc:585] Rank 0:: Started instantiating SUT[llama2-70b-Offline-SUT Rank0]
I0228 16:43:48.844092 21351 main_llm.cc:506] Rank 1:: local session [rank/size | color | devId]: [1/2 | 0 | 1]
I0228 16:43:48.844105 21351 main_llm.cc:585] Rank 1:: Started instantiating SUT[llama2-70b-Offline-SUT Rank1]
I0228 16:43:48.844094 21352 main_llm.cc:506] Rank 2:: local session [rank/size | color | devId]: [0/1 | 1 | -]
I0228 16:43:48.844106 21352 main_llm.cc:529] Rank 2:: Started instantiating QSL[llama2-70b-Offline-QSL-LoadGen]
I0228 16:43:48.844341 21350 llm_server.cc:406] LLMServer[llama2-70b-Offline-SUT Rank0] creating LLMCore[0] on Device[0]...
I0228 16:43:48.844369 21350 llm_core.cc:68] Rank 0:: LLMCore[0] at Device Id [0]
[TensorRT-LLM][INFO] Engine version 0.9.0.dev2024020600 found in the config file, assuming engine(s) built by new builder API.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'lora_target_modules' not found
[TensorRT-LLM][WARNING] Optional value for parameter lora_target_modules will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'num_medusa_heads' not found
[TensorRT-LLM][WARNING] Optional value for parameter num_medusa_heads will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'max_draft_len' not found
[TensorRT-LLM][WARNING] Optional value for parameter max_draft_len will not be set.
[TensorRT-LLM][INFO] MPI size: 2, rank: 0
[TensorRT-LLM][INFO] Using user-specified devices: (0, 1)
[TensorRT-LLM][INFO] Rank 0 is using GPU 0
I0228 16:43:48.891845 21352 main_llm.cc:532] Rank 2:: Finished instantiating QSL[llama2-70b-Offline-QSL-LoadGen]
I0228 16:43:48.891872 21352 main_llm.cc:534] Rank 2:: Started instantiating SUT[llama2-70b-Offline-SUT-LoadGen]
I0228 16:43:48.902535 21352 main_llm.cc:539] Rank 2:: Finished instantiating SUT[llama2-70b-Offline-SUT-LoadGen]
I0228 16:43:48.902550 21352 main_llm.cc:541] Rank 2:: Waiting for the end of init phase of other nodes...
I0228 16:43:49.460567 21351 llm_server.cc:406] LLMServer[llama2-70b-Offline-SUT Rank1] creating LLMCore[0] on Device[1]...
I0228 16:43:49.460611 21351 llm_core.cc:68] Rank 1:: LLMCore[0] at Device Id [1]
[TensorRT-LLM][INFO] Engine version 0.9.0.dev2024020600 found in the config file, assuming engine(s) built by new builder API.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'lora_target_modules' not found
[TensorRT-LLM][WARNING] Optional value for parameter lora_target_modules will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'num_medusa_heads' not found
[TensorRT-LLM][WARNING] Optional value for parameter num_medusa_heads will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'max_draft_len' not found
[TensorRT-LLM][WARNING] Optional value for parameter max_draft_len will not be set.
[TensorRT-LLM][INFO] MPI size: 2, rank: 1
[TensorRT-LLM][INFO] Using user-specified devices: (0, 1)
[TensorRT-LLM][INFO] Rank 1 is using GPU 1
[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 1300
[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 1300
[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: 2048
[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0
[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1
[TensorRT-LLM][INFO] Loaded engine size: 33169 MiB
[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 1300
[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 1300
[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: 2048
[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0
[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1
[TensorRT-LLM][INFO] Loaded engine size: 33170 MiB
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +66, now: CPU 33243, GPU 35337 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +2, GPU +72, now: CPU 33245, GPU 35409 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +66, now: CPU 33260, GPU 33750 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +2, GPU +72, now: CPU 33262, GPU 33822 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +33142, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +33142, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] [MS] Running engine with multi stream info
[TensorRT-LLM][INFO] [MS] Number of aux streams is 1
[TensorRT-LLM][INFO] [MS] Number of total worker streams is 2
[TensorRT-LLM][INFO] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 33538, GPU 34914 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 33538, GPU 34978 (MiB)
[TensorRT-LLM][INFO] [MS] Running engine with multi stream info
[TensorRT-LLM][INFO] [MS] Number of aux streams is 1
[TensorRT-LLM][INFO] [MS] Number of total worker streams is 2
[TensorRT-LLM][INFO] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 33521, GPU 36501 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 33521, GPU 36565 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] Allocate 53036974080 bytes for k/v cache. 
[TensorRT-LLM][INFO] Using 647424 total tokens in paged KV cache, and 16 blocks per sequence
[TensorRT-LLM][INFO] Allocate 53036974080 bytes for k/v cache. 
[TensorRT-LLM][INFO] Using 647424 total tokens in paged KV cache, and 16 blocks per sequence
I0228 16:44:47.444136 21350 llm_core.cc:91] Rank 0:: LLMCore Setup complete for device 0
I0228 16:44:47.444207 21350 llm_server.cc:411] LLMServer[llama2-70b-Offline-SUT Rank0] created LLMCore[0] on Device[0].
I0228 16:44:47.453620 21350 main_llm.cc:592] Rank 0:: Finished instantiating SUT[llama2-70b-Offline-SUT Rank0]
I0228 16:44:47.453634 21350 main_llm.cc:593] Rank 0:: Started Leader SUT
I0228 16:44:48.133190 21351 llm_core.cc:91] Rank 1:: LLMCore Setup complete for device 1
I0228 16:44:48.133258 21351 llm_server.cc:411] LLMServer[llama2-70b-Offline-SUT Rank1] created LLMCore[0] on Device[1].
I0228 16:44:48.133266 21351 main_llm.cc:599] Rank 1:: Finished instantiating SUT[llama2-70b-Offline-SUT Rank1]
I0228 16:44:48.133298 21351 main_llm.cc:600] Rank 1:: Started follower SUT
I0228 16:44:48.133456 21351 main_llm.cc:606] Rank 1:: Test should start now.
I0228 16:44:48.133491 21350 main_llm.cc:606] Rank 0:: Test should start now.
I0228 16:44:48.133513 21352 main_llm.cc:544] Rank 2:: Init phase done on all the nodes.
I0228 16:44:48.133582 21352 main_llm.cc:546] Rank 2:: Starting running actual test.
================================================
MLPerf Results Summary
================================================
SUT name : llama2-70b-Offline-SUT-LoadGen
Scenario : Offline
Mode     : PerformanceOnly
Samples per second: 3.12965
Tokens per second: 955.014
Result is : VALID
  Min duration satisfied : Yes
  Min queries satisfied : Yes
  Early stopping satisfied: Yes

================================================
Additional Stats
================================================
Min latency (ns)                : 3844454886
Max latency (ns)                : 31952408395
Mean latency (ns)               : 12172009280
50.00 percentile latency (ns)   : 11393941897
90.00 percentile latency (ns)   : 20236735408
95.00 percentile latency (ns)   : 22262775117
97.00 percentile latency (ns)   : 23993264186
99.00 percentile latency (ns)   : 31952408395
99.90 percentile latency (ns)   : 31952408395


================================================
Test Parameters Used
================================================
samples_per_query : 100
target_qps : 13.5
ttft_latency (ns): 2000000000
tpot_latency (ns): 200000000
max_async_queries : 1
min_duration (ms): 0
max_duration (ms): 0
min_query_count : 1
max_query_count : 0
qsl_rng_seed : 13281865557512327830
sample_index_rng_seed : 198141574272810017
schedule_rng_seed : 7575108116881280410
accuracy_log_rng_seed : 720381539243781796
accuracy_log_probability : 0
accuracy_log_sampling_target : 100
print_timestamps : 0
performance_issue_unique : 0
performance_issue_same : 0
performance_issue_same_index : 0
performance_sample_count : 24576

1 warning encountered. See detailed log.

No errors encountered during test.
I0228 16:45:20.181702 21352 main_llm.cc:548] Rank 2:: Finished running actual test.
I0228 16:45:20.181744 21352 main_llm.cc:550] Rank 2:: Initiating termination sequence.
I0228 16:45:20.181758 21352 main_llm.cc:552] Rank 2:: Termination sequence initiated, waiting for others...
I0228 16:45:20.181835 21372 llm_server.cc:1029] Rank 2:: llama2-70b-Offline-SUT-LoadGen Port[0] unblocked from RECV for shut-down
I0228 16:45:20.181865 21372 llm_server.cc:953] Rank 2:: llama2-70b-Offline-SUT-LoadGen Stopping...
I0228 16:45:20.181851 21380 llm_server.cc:584] Rank 0:: llama2-70b-Offline-SUT Rank0 unblocked from RECV to shut-down
I0228 16:45:21.181955 21352 main_llm.cc:557] Rank 2:: Sync'ed with others for shutting-off.
I0228 16:45:21.181965 21351 main_llm.cc:611] Rank 1:: Test should finish now.
I0228 16:45:21.182011 21350 main_llm.cc:611] Rank 0:: Test should finish now.
I0228 16:45:21.200553 21352 main_llm.cc:618] Rank 2:: All done
[TensorRT-LLM][INFO] Terminate signal received, worker thread exiting.
[TensorRT-LLM][INFO] Terminate signal received, worker thread exiting.
I0228 16:45:21.773797 21350 main_llm.cc:618] Rank 0:: All done
I0228 16:45:21.788904 21351 main_llm.cc:618] Rank 1:: All done
[2024-02-28 16:45:23,474 run_harness.py:165 INFO] Result: result_tokens_per_second: 955.014, Result is VALID
[2024-02-28 16:45:23,474 run_audit.py:162 INFO] => Running compliance harness verification script...
[2024-02-28 16:45:23,474 __init__.py:46 INFO] Running command: /usr/bin/python3.8 /work/build/inference/compliance/nvidia/TEST06/run_verification.py --compliance_dir=build/compliance_logs/TEST06/NC_H100_v5_TRT/llama2-70b-99/Offline --output_dir=build/compliance_logs/TEST06/NC_H100_v5_TRT/llama2-70b-99/Offline --scenario=Offline --dtype=int32
First token check pass: Skipped
EOS check pass: True
Sample length check pass: True
TEST06 verification complete

[2024-02-28 16:45:23,570 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:45:23,570 auditing.py:156 INFO] Audit cleanup: Removing file audit.config
[2024-02-28 16:45:23,570 run_audit.py:320 INFO] => Submission checker: Audit test TEST06 PASS
[2024-02-28 16:45:23,580 run_audit.py:142 INFO] => Running compliance harness for test TEST06
[2024-02-28 16:45:23,580 auditing.py:139 INFO] AUDIT HARNESS: Looking for audit.config in build/inference/compliance/nvidia/TEST06/llama2-70b/audit.config...
[2024-02-28 16:45:23,580 auditing.py:143 INFO] AUDIT HARNESS: Search failed. Looking for audit.config in build/inference/compliance/nvidia/TEST06/audit.config...
[2024-02-28 16:45:23,581 generate_conf_files.py:107 INFO] Generated measurements/ entries for NC_H100_v5_TRT/llama2-70b-99.9/Offline
[2024-02-28 16:45:23,581 __init__.py:46 INFO] Running command: mpirun -np 3 ./build/bin/harness_llm --logfile_outdir="build/compliance_logs/TEST06/NC_H100_v5_TRT/llama2-70b-99.9/Offline" --logfile_prefix="mlperf_log_" --performance_sample_count=24576 --gpu_batch_size=1300 --tensor_path="build/preprocessed_data/open_orca/input_ids_padded.npy,build/preprocessed_data/open_orca/input_lens.npy" --use_graphs=false --gpu_inference_streams=1 --gpu_copy_streams=1 --tensor_parallelism=2 --pipeline_parallelism=1 --kvcache_free_gpu_mem_frac=0.9 --enable_sort=false --llm_gen_config_path="code/llama2-70b/tensorrt/generation_config.json" --use_token_latencies=true --gpu_engines="./build/engines/NC_H100_v5/llama2-70b/Offline/bs1300-custom_k_99_9_MaxP-tp2-pp1/rank0.engine" --mlperf_conf_path="build/loadgen-configs/NC_H100_v5_TRT/llama2-70b-99.9/Offline/mlperf.conf" --user_conf_path="build/loadgen-configs/NC_H100_v5_TRT/llama2-70b-99.9/Offline/user.conf" --scenario Offline --model llama2-70b
[2024-02-28 16:45:23,581 __init__.py:53 INFO] Overriding Environment
benchmark : Benchmark.LLAMA2
buffer_manager_thread_count : 0
coalesced_tensor : True
data_dir : /mnt/resource_nvme/scratch/data
enable_sort : False
gpu_batch_size : 1300
gpu_copy_streams : 1
gpu_inference_streams : 1
input_dtype : int32
input_format : linear
kvcache_free_gpu_mem_frac : 0.9
llm_gen_config_path : code/llama2-70b/tensorrt/generation_config.json
log_dir : build/compliance_logs/TEST06
offline_expected_qps : 13.5
pipeline_parallelism : 1
precision : fp16
preprocessed_data_dir : /mnt/resource_nvme/scratch/preprocessed_data
scenario : Scenario.Offline
system : SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='AMD EPYC 9V84 96-Core Processor', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=80, threads_per_core=1): 1}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=660.463936, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=660463936000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA H100 NVL', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=93.583984375, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=100485038080), max_power_limit=400.0, pci_id='0x232110DE', compute_sm=90): 2})), numa_conf=NUMAConfiguration(numa_nodes={}, num_numa_nodes=2), system_id='NC_H100_v5')
tensor_parallelism : 2
tensor_path : build/preprocessed_data/open_orca/input_ids_padded.npy,build/preprocessed_data/open_orca/input_lens.npy
use_fp8 : True
use_graphs : False
use_token_latencies : True
system_id : NC_H100_v5
config_name : NC_H100_v5_llama2-70b_Offline
workload_setting : WorkloadSetting(HarnessType.Custom, AccuracyTarget.k_99_9, PowerSetting.MaxP)
optimization_level : plugin-enabled
num_profiles : 1
config_ver : custom_k_99_9_MaxP
accuracy_level : 99.9%
inference_server : custom
audit_test_name : TEST06
skip_file_checks : False
power_limit : None
cpu_freq : None
&&&& RUNNING LLM_HARNESS # ./build/bin/harness_llm
[I] Initializing TensorRT plugin library...
[I] Initializing TRT-LLM plugin libraries...
&&&& RUNNING LLM_HARNESS # ./build/bin/harness_llm
[I] Initializing TensorRT plugin library...
[I] Initializing TRT-LLM plugin libraries...
&&&& RUNNING LLM_HARNESS # ./build/bin/harness_llm
[I] Initializing TensorRT plugin library...
[I] Initializing TRT-LLM plugin libraries...
I0228 16:45:26.953398 21456 main_llm.cc:170] Loading LLM config from "./build/engines/NC_H100_v5/llama2-70b/Offline/bs1300-custom_k_99_9_MaxP-tp2-pp1/config.json" and geneartion config from: code/llama2-70b/tensorrt/generation_config.json
I0228 16:45:27.017187 21457 main_llm.cc:170] Loading LLM config from "./build/engines/NC_H100_v5/llama2-70b/Offline/bs1300-custom_k_99_9_MaxP-tp2-pp1/config.json" and geneartion config from: code/llama2-70b/tensorrt/generation_config.json
I0228 16:45:27.023361 21455 main_llm.cc:170] Loading LLM config from "./build/engines/NC_H100_v5/llama2-70b/Offline/bs1300-custom_k_99_9_MaxP-tp2-pp1/config.json" and geneartion config from: code/llama2-70b/tensorrt/generation_config.json
I0228 16:45:27.251688 21456 main_llm.cc:284] Rank: 1 has pid: 21456
I0228 16:45:27.251681 21457 main_llm.cc:284] Rank: 2 has pid: 21457
I0228 16:45:27.251704 21457 main_llm.cc:121] Found 2 GPUs
I0228 16:45:27.251710 21456 main_llm.cc:121] Found 2 GPUs
I0228 16:45:27.251708 21455 main_llm.cc:281] LLMConfig Details:
	mMaxSumSeqlen: 2048 mMaxInputSeqlen: 1024 mMaxOutputSeqlen: 1024 mEosId: 2
	tp: 2 pp: 1 mWorldSize: 2
	mMaxGpuBatchSize: 1300 mBeamWidth: 1
	mTemperature: 1 mTopK: 1 mTopP: 0 mMinOutputSeqlen: 1 mIsStreaming: 0 mReportFirstToken: 0 mEnableSort: 0 mExcludeInputInOutput: 1
	mMaxNumSequences: 0 mMaxTokensInPagedKvcache: 0 mKvCacheFreeGpuMemFraction: 0.9 mEnableTrtOverlap: 0
	mBatchMode: 2 mSchedulerPolicy: 0
I0228 16:45:27.251734 21455 main_llm.cc:284] Rank: 0 has pid: 21455
I0228 16:45:27.251739 21455 main_llm.cc:121] Found 2 GPUs
I0228 16:45:27.251893 21456 main_llm.cc:506] Rank 1:: local session [rank/size | color | devId]: [1/2 | 0 | 1]
I0228 16:45:27.251904 21456 main_llm.cc:585] Rank 1:: Started instantiating SUT[llama2-70b-Offline-SUT Rank1]
I0228 16:45:27.251891 21457 main_llm.cc:506] Rank 2:: local session [rank/size | color | devId]: [0/1 | 1 | -]
I0228 16:45:27.251901 21457 main_llm.cc:529] Rank 2:: Started instantiating QSL[llama2-70b-Offline-QSL-LoadGen]
I0228 16:45:27.251891 21455 main_llm.cc:506] Rank 0:: local session [rank/size | color | devId]: [0/2 | 0 | 0]
I0228 16:45:27.251902 21455 main_llm.cc:585] Rank 0:: Started instantiating SUT[llama2-70b-Offline-SUT Rank0]
I0228 16:45:27.252159 21455 llm_server.cc:406] LLMServer[llama2-70b-Offline-SUT Rank0] creating LLMCore[0] on Device[0]...
I0228 16:45:27.252168 21455 llm_core.cc:68] Rank 0:: LLMCore[0] at Device Id [0]
[TensorRT-LLM][INFO] Engine version 0.9.0.dev2024020600 found in the config file, assuming engine(s) built by new builder API.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'lora_target_modules' not found
[TensorRT-LLM][WARNING] Optional value for parameter lora_target_modules will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'num_medusa_heads' not found
[TensorRT-LLM][WARNING] Optional value for parameter num_medusa_heads will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'max_draft_len' not found
[TensorRT-LLM][WARNING] Optional value for parameter max_draft_len will not be set.
[TensorRT-LLM][INFO] MPI size: 2, rank: 0
[TensorRT-LLM][INFO] Using user-specified devices: (0, 1)
[TensorRT-LLM][INFO] Rank 0 is using GPU 0
I0228 16:45:27.295059 21457 main_llm.cc:532] Rank 2:: Finished instantiating QSL[llama2-70b-Offline-QSL-LoadGen]
I0228 16:45:27.295089 21457 main_llm.cc:534] Rank 2:: Started instantiating SUT[llama2-70b-Offline-SUT-LoadGen]
I0228 16:45:27.305876 21457 main_llm.cc:539] Rank 2:: Finished instantiating SUT[llama2-70b-Offline-SUT-LoadGen]
I0228 16:45:27.305893 21457 main_llm.cc:541] Rank 2:: Waiting for the end of init phase of other nodes...
I0228 16:45:27.876937 21456 llm_server.cc:406] LLMServer[llama2-70b-Offline-SUT Rank1] creating LLMCore[0] on Device[1]...
I0228 16:45:27.876983 21456 llm_core.cc:68] Rank 1:: LLMCore[0] at Device Id [1]
[TensorRT-LLM][INFO] Engine version 0.9.0.dev2024020600 found in the config file, assuming engine(s) built by new builder API.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'lora_target_modules' not found
[TensorRT-LLM][WARNING] Optional value for parameter lora_target_modules will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'num_medusa_heads' not found
[TensorRT-LLM][WARNING] Optional value for parameter num_medusa_heads will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'max_draft_len' not found
[TensorRT-LLM][WARNING] Optional value for parameter max_draft_len will not be set.
[TensorRT-LLM][INFO] MPI size: 2, rank: 1
[TensorRT-LLM][INFO] Using user-specified devices: (0, 1)
[TensorRT-LLM][INFO] Rank 1 is using GPU 1
[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 1300
[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 1300
[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: 2048
[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0
[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1
[TensorRT-LLM][INFO] Loaded engine size: 33170 MiB
[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 1300
[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 1300
[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: 2048
[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0
[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1
[TensorRT-LLM][INFO] Loaded engine size: 33170 MiB
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +66, now: CPU 33244, GPU 35337 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +2, GPU +72, now: CPU 33246, GPU 35409 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +66, now: CPU 33260, GPU 33750 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +2, GPU +72, now: CPU 33262, GPU 33822 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +33142, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +33142, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] [MS] Running engine with multi stream info
[TensorRT-LLM][INFO] [MS] Number of aux streams is 1
[TensorRT-LLM][INFO] [MS] Number of total worker streams is 2
[TensorRT-LLM][INFO] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 33538, GPU 34914 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 33538, GPU 34978 (MiB)
[TensorRT-LLM][INFO] [MS] Running engine with multi stream info
[TensorRT-LLM][INFO] [MS] Number of aux streams is 1
[TensorRT-LLM][INFO] [MS] Number of total worker streams is 2
[TensorRT-LLM][INFO] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 33522, GPU 36501 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 33522, GPU 36565 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] Allocate 53036974080 bytes for k/v cache. 
[TensorRT-LLM][INFO] Using 647424 total tokens in paged KV cache, and 16 blocks per sequence
[TensorRT-LLM][INFO] Allocate 53036974080 bytes for k/v cache. 
[TensorRT-LLM][INFO] Using 647424 total tokens in paged KV cache, and 16 blocks per sequence
I0228 16:46:26.088618 21455 llm_core.cc:91] Rank 0:: LLMCore Setup complete for device 0
I0228 16:46:26.088686 21455 llm_server.cc:411] LLMServer[llama2-70b-Offline-SUT Rank0] created LLMCore[0] on Device[0].
I0228 16:46:26.098098 21455 main_llm.cc:592] Rank 0:: Finished instantiating SUT[llama2-70b-Offline-SUT Rank0]
I0228 16:46:26.098110 21455 main_llm.cc:593] Rank 0:: Started Leader SUT
I0228 16:46:26.266554 21456 llm_core.cc:91] Rank 1:: LLMCore Setup complete for device 1
I0228 16:46:26.266623 21456 llm_server.cc:411] LLMServer[llama2-70b-Offline-SUT Rank1] created LLMCore[0] on Device[1].
I0228 16:46:26.266659 21456 main_llm.cc:599] Rank 1:: Finished instantiating SUT[llama2-70b-Offline-SUT Rank1]
I0228 16:46:26.266662 21456 main_llm.cc:600] Rank 1:: Started follower SUT
I0228 16:46:26.266811 21456 main_llm.cc:606] Rank 1:: Test should start now.
I0228 16:46:26.266839 21455 main_llm.cc:606] Rank 0:: Test should start now.
I0228 16:46:26.266870 21457 main_llm.cc:544] Rank 2:: Init phase done on all the nodes.
I0228 16:46:26.266934 21457 main_llm.cc:546] Rank 2:: Starting running actual test.
================================================
MLPerf Results Summary
================================================
SUT name : llama2-70b-Offline-SUT-LoadGen
Scenario : Offline
Mode     : PerformanceOnly
Samples per second: 3.16578
Tokens per second: 966.037
Result is : VALID
  Min duration satisfied : Yes
  Min queries satisfied : Yes
  Early stopping satisfied: Yes

================================================
Additional Stats
================================================
Min latency (ns)                : 3852906313
Max latency (ns)                : 31587820909
Mean latency (ns)               : 12074808176
50.00 percentile latency (ns)   : 11309851609
90.00 percentile latency (ns)   : 20025679103
95.00 percentile latency (ns)   : 22024552603
97.00 percentile latency (ns)   : 23725563333
99.00 percentile latency (ns)   : 31587820909
99.90 percentile latency (ns)   : 31587820909


================================================
Test Parameters Used
================================================
samples_per_query : 100
target_qps : 13.5
ttft_latency (ns): 2000000000
tpot_latency (ns): 200000000
max_async_queries : 1
min_duration (ms): 0
max_duration (ms): 0
min_query_count : 1
max_query_count : 0
qsl_rng_seed : 13281865557512327830
sample_index_rng_seed : 198141574272810017
schedule_rng_seed : 7575108116881280410
accuracy_log_rng_seed : 720381539243781796
accuracy_log_probability : 0
accuracy_log_sampling_target : 100
print_timestamps : 0
performance_issue_unique : 0
performance_issue_same : 0
performance_issue_same_index : 0
performance_sample_count : 24576

1 warning encountered. See detailed log.

No errors encountered during test.
I0228 16:46:57.944217 21457 main_llm.cc:548] Rank 2:: Finished running actual test.
I0228 16:46:57.944259 21457 main_llm.cc:550] Rank 2:: Initiating termination sequence.
I0228 16:46:57.944275 21457 main_llm.cc:552] Rank 2:: Termination sequence initiated, waiting for others...
I0228 16:46:57.944358 21477 llm_server.cc:1029] Rank 2:: llama2-70b-Offline-SUT-LoadGen Port[0] unblocked from RECV for shut-down
I0228 16:46:57.944388 21477 llm_server.cc:953] Rank 2:: llama2-70b-Offline-SUT-LoadGen Stopping...
I0228 16:46:57.944372 21485 llm_server.cc:584] Rank 0:: llama2-70b-Offline-SUT Rank0 unblocked from RECV to shut-down
I0228 16:46:58.944439 21457 main_llm.cc:557] Rank 2:: Sync'ed with others for shutting-off.
I0228 16:46:58.944481 21455 main_llm.cc:611] Rank 0:: Test should finish now.
I0228 16:46:58.944469 21456 main_llm.cc:611] Rank 1:: Test should finish now.
I0228 16:46:58.962785 21457 main_llm.cc:618] Rank 2:: All done
[TensorRT-LLM][INFO] Terminate signal received, worker thread exiting.
[TensorRT-LLM][INFO] Terminate signal received, worker thread exiting.
I0228 16:46:59.486253 21455 main_llm.cc:618] Rank 0:: All done
I0228 16:46:59.501014 21456 main_llm.cc:618] Rank 1:: All done
[2024-02-28 16:47:01,215 run_harness.py:165 INFO] Result: result_tokens_per_second: 966.037, Result is VALID
[2024-02-28 16:47:01,215 run_audit.py:162 INFO] => Running compliance harness verification script...
[2024-02-28 16:47:01,215 __init__.py:46 INFO] Running command: /usr/bin/python3.8 /work/build/inference/compliance/nvidia/TEST06/run_verification.py --compliance_dir=build/compliance_logs/TEST06/NC_H100_v5_TRT/llama2-70b-99.9/Offline --output_dir=build/compliance_logs/TEST06/NC_H100_v5_TRT/llama2-70b-99.9/Offline --scenario=Offline --dtype=int32
First token check pass: Skipped
EOS check pass: True
Sample length check pass: True
TEST06 verification complete

[2024-02-28 16:47:01,307 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:47:01,308 auditing.py:156 INFO] Audit cleanup: Removing file audit.config
[2024-02-28 16:47:01,308 run_audit.py:320 INFO] => Submission checker: Audit test TEST06 PASS
[2024-02-28 16:47:01,317 run_audit.py:142 INFO] => Running compliance harness for test TEST06
[2024-02-28 16:47:01,317 auditing.py:139 INFO] AUDIT HARNESS: Looking for audit.config in build/inference/compliance/nvidia/TEST06/llama2-70b/audit.config...
[2024-02-28 16:47:01,317 auditing.py:143 INFO] AUDIT HARNESS: Search failed. Looking for audit.config in build/inference/compliance/nvidia/TEST06/audit.config...
[2024-02-28 16:47:01,318 generate_conf_files.py:107 INFO] Generated measurements/ entries for NC_H100_v5_TRT/llama2-70b-99/Server
[2024-02-28 16:47:01,318 __init__.py:46 INFO] Running command: mpirun -np 3 ./build/bin/harness_llm --logfile_outdir="build/compliance_logs/TEST06/NC_H100_v5_TRT/llama2-70b-99/Server" --logfile_prefix="mlperf_log_" --performance_sample_count=24576 --gpu_batch_size=1536 --tensor_path="build/preprocessed_data/open_orca/input_ids_padded.npy,build/preprocessed_data/open_orca/input_lens.npy" --use_graphs=false --gpu_inference_streams=1 --gpu_copy_streams=1 --tensor_parallelism=2 --pipeline_parallelism=1 --kvcache_free_gpu_mem_frac=0.9 --enable_sort=false --llm_gen_config_path="code/llama2-70b/tensorrt/generation_config.json" --use_token_latencies=true --gpu_engines="./build/engines/NC_H100_v5/llama2-70b/Server/bs1536-custom_k_99_MaxP-tp2-pp1/rank0.engine" --mlperf_conf_path="build/loadgen-configs/NC_H100_v5_TRT/llama2-70b-99/Server/mlperf.conf" --user_conf_path="build/loadgen-configs/NC_H100_v5_TRT/llama2-70b-99/Server/user.conf" --scenario Server --model llama2-70b
[2024-02-28 16:47:01,318 __init__.py:53 INFO] Overriding Environment
benchmark : Benchmark.LLAMA2
buffer_manager_thread_count : 0
coalesced_tensor : True
data_dir : /mnt/resource_nvme/scratch/data
enable_sort : False
gpu_batch_size : 1536
gpu_copy_streams : 1
gpu_inference_streams : 1
input_dtype : int32
input_format : linear
kvcache_free_gpu_mem_frac : 0.9
llm_gen_config_path : code/llama2-70b/tensorrt/generation_config.json
log_dir : build/compliance_logs/TEST06
pipeline_parallelism : 1
precision : fp16
preprocessed_data_dir : /mnt/resource_nvme/scratch/preprocessed_data
scenario : Scenario.Server
server_target_qps : 12
server_target_qps_adj_factor : 0.92
system : SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='AMD EPYC 9V84 96-Core Processor', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=80, threads_per_core=1): 1}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=660.463936, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=660463936000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA H100 NVL', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=93.583984375, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=100485038080), max_power_limit=400.0, pci_id='0x232110DE', compute_sm=90): 2})), numa_conf=NUMAConfiguration(numa_nodes={}, num_numa_nodes=2), system_id='NC_H100_v5')
tensor_parallelism : 2
tensor_path : build/preprocessed_data/open_orca/input_ids_padded.npy,build/preprocessed_data/open_orca/input_lens.npy
use_fp8 : True
use_graphs : False
use_token_latencies : True
system_id : NC_H100_v5
config_name : NC_H100_v5_llama2-70b_Server
workload_setting : WorkloadSetting(HarnessType.Custom, AccuracyTarget.k_99, PowerSetting.MaxP)
optimization_level : plugin-enabled
num_profiles : 1
config_ver : custom_k_99_MaxP
accuracy_level : 99%
inference_server : custom
audit_test_name : TEST06
skip_file_checks : False
power_limit : None
cpu_freq : None
&&&& RUNNING LLM_HARNESS # ./build/bin/harness_llm
[I] Initializing TensorRT plugin library...
[I] Initializing TRT-LLM plugin libraries...
&&&& RUNNING LLM_HARNESS # ./build/bin/harness_llm
[I] Initializing TensorRT plugin library...
[I] Initializing TRT-LLM plugin libraries...
&&&& RUNNING LLM_HARNESS # ./build/bin/harness_llm
[I] Initializing TensorRT plugin library...
[I] Initializing TRT-LLM plugin libraries...
I0228 16:47:04.719727 21561 main_llm.cc:170] Loading LLM config from "./build/engines/NC_H100_v5/llama2-70b/Server/bs1536-custom_k_99_MaxP-tp2-pp1/config.json" and geneartion config from: code/llama2-70b/tensorrt/generation_config.json
I0228 16:47:04.747859 21562 main_llm.cc:170] Loading LLM config from "./build/engines/NC_H100_v5/llama2-70b/Server/bs1536-custom_k_99_MaxP-tp2-pp1/config.json" and geneartion config from: code/llama2-70b/tensorrt/generation_config.json
I0228 16:47:04.764401 21560 main_llm.cc:170] Loading LLM config from "./build/engines/NC_H100_v5/llama2-70b/Server/bs1536-custom_k_99_MaxP-tp2-pp1/config.json" and geneartion config from: code/llama2-70b/tensorrt/generation_config.json
I0228 16:47:04.985386 21561 main_llm.cc:284] Rank: 1 has pid: 21561
I0228 16:47:04.985412 21561 main_llm.cc:121] Found 2 GPUs
I0228 16:47:04.985391 21562 main_llm.cc:284] Rank: 2 has pid: 21562
I0228 16:47:04.985414 21562 main_llm.cc:121] Found 2 GPUs
I0228 16:47:04.985479 21560 main_llm.cc:281] LLMConfig Details:
	mMaxSumSeqlen: 2048 mMaxInputSeqlen: 1024 mMaxOutputSeqlen: 1024 mEosId: 2
	tp: 2 pp: 1 mWorldSize: 2
	mMaxGpuBatchSize: 1536 mBeamWidth: 1
	mTemperature: 1 mTopK: 1 mTopP: 0 mMinOutputSeqlen: 1 mIsStreaming: 1 mReportFirstToken: 1 mEnableSort: 0 mExcludeInputInOutput: 1
	mMaxNumSequences: 0 mMaxTokensInPagedKvcache: 0 mKvCacheFreeGpuMemFraction: 0.9 mEnableTrtOverlap: 0
	mBatchMode: 2 mSchedulerPolicy: 0
I0228 16:47:04.985508 21560 main_llm.cc:284] Rank: 0 has pid: 21560
I0228 16:47:04.985518 21560 main_llm.cc:121] Found 2 GPUs
I0228 16:47:04.985692 21562 main_llm.cc:506] Rank 2:: local session [rank/size | color | devId]: [0/1 | 1 | -]
I0228 16:47:04.985699 21562 main_llm.cc:529] Rank 2:: Started instantiating QSL[llama2-70b-Server-QSL-LoadGen]
I0228 16:47:04.985685 21561 main_llm.cc:506] Rank 1:: local session [rank/size | color | devId]: [1/2 | 0 | 1]
I0228 16:47:04.985697 21561 main_llm.cc:585] Rank 1:: Started instantiating SUT[llama2-70b-Server-SUT Rank1]
I0228 16:47:04.985685 21560 main_llm.cc:506] Rank 0:: local session [rank/size | color | devId]: [0/2 | 0 | 0]
I0228 16:47:04.985697 21560 main_llm.cc:585] Rank 0:: Started instantiating SUT[llama2-70b-Server-SUT Rank0]
I0228 16:47:04.985910 21560 llm_server.cc:406] LLMServer[llama2-70b-Server-SUT Rank0] creating LLMCore[0] on Device[0]...
I0228 16:47:04.985919 21560 llm_core.cc:68] Rank 0:: LLMCore[0] at Device Id [0]
[TensorRT-LLM][INFO] Engine version 0.9.0.dev2024020600 found in the config file, assuming engine(s) built by new builder API.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'lora_target_modules' not found
[TensorRT-LLM][WARNING] Optional value for parameter lora_target_modules will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'num_medusa_heads' not found
[TensorRT-LLM][WARNING] Optional value for parameter num_medusa_heads will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'max_draft_len' not found
[TensorRT-LLM][WARNING] Optional value for parameter max_draft_len will not be set.
[TensorRT-LLM][INFO] MPI size: 2, rank: 0
[TensorRT-LLM][INFO] Using user-specified devices: (0, 1)
[TensorRT-LLM][INFO] Rank 0 is using GPU 0
I0228 16:47:05.046248 21562 main_llm.cc:532] Rank 2:: Finished instantiating QSL[llama2-70b-Server-QSL-LoadGen]
I0228 16:47:05.046277 21562 main_llm.cc:534] Rank 2:: Started instantiating SUT[llama2-70b-Server-SUT-LoadGen]
I0228 16:47:05.057374 21562 main_llm.cc:539] Rank 2:: Finished instantiating SUT[llama2-70b-Server-SUT-LoadGen]
I0228 16:47:05.057389 21562 main_llm.cc:541] Rank 2:: Waiting for the end of init phase of other nodes...
I0228 16:47:05.620828 21561 llm_server.cc:406] LLMServer[llama2-70b-Server-SUT Rank1] creating LLMCore[0] on Device[1]...
I0228 16:47:05.620869 21561 llm_core.cc:68] Rank 1:: LLMCore[0] at Device Id [1]
[TensorRT-LLM][INFO] Engine version 0.9.0.dev2024020600 found in the config file, assuming engine(s) built by new builder API.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'lora_target_modules' not found
[TensorRT-LLM][WARNING] Optional value for parameter lora_target_modules will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'num_medusa_heads' not found
[TensorRT-LLM][WARNING] Optional value for parameter num_medusa_heads will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'max_draft_len' not found
[TensorRT-LLM][WARNING] Optional value for parameter max_draft_len will not be set.
[TensorRT-LLM][INFO] MPI size: 2, rank: 1
[TensorRT-LLM][INFO] Using user-specified devices: (0, 1)
[TensorRT-LLM][INFO] Rank 1 is using GPU 1
[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 1536
[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 1536
[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: 2048
[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0
[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1
[TensorRT-LLM][INFO] Loaded engine size: 33176 MiB
[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 1536
[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 1536
[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: 2048
[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0
[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1
[TensorRT-LLM][INFO] Loaded engine size: 33176 MiB
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +66, now: CPU 33257, GPU 35337 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +2, GPU +72, now: CPU 33259, GPU 35409 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +66, now: CPU 33273, GPU 33750 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +2, GPU +72, now: CPU 33275, GPU 33822 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +33142, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +33142, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] [MS] Running engine with multi stream info
[TensorRT-LLM][INFO] [MS] Number of aux streams is 1
[TensorRT-LLM][INFO] [MS] Number of total worker streams is 2
[TensorRT-LLM][INFO] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 33551, GPU 34914 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 33551, GPU 34978 (MiB)
[TensorRT-LLM][INFO] [MS] Running engine with multi stream info
[TensorRT-LLM][INFO] [MS] Number of aux streams is 1
[TensorRT-LLM][INFO] [MS] Number of total worker streams is 2
[TensorRT-LLM][INFO] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 33535, GPU 36501 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 33535, GPU 36565 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] Allocate 52617543680 bytes for k/v cache. 
[TensorRT-LLM][INFO] Using 642304 total tokens in paged KV cache, and 16 blocks per sequence
[TensorRT-LLM][INFO] Allocate 52617543680 bytes for k/v cache. 
[TensorRT-LLM][INFO] Using 642304 total tokens in paged KV cache, and 16 blocks per sequence
I0228 16:48:03.525523 21560 llm_core.cc:91] Rank 0:: LLMCore Setup complete for device 0
I0228 16:48:03.525590 21560 llm_server.cc:411] LLMServer[llama2-70b-Server-SUT Rank0] created LLMCore[0] on Device[0].
I0228 16:48:03.534472 21560 main_llm.cc:592] Rank 0:: Finished instantiating SUT[llama2-70b-Server-SUT Rank0]
I0228 16:48:03.534485 21560 main_llm.cc:593] Rank 0:: Started Leader SUT
I0228 16:48:04.110869 21561 llm_core.cc:91] Rank 1:: LLMCore Setup complete for device 1
I0228 16:48:04.110937 21561 llm_server.cc:411] LLMServer[llama2-70b-Server-SUT Rank1] created LLMCore[0] on Device[1].
I0228 16:48:04.110945 21561 main_llm.cc:599] Rank 1:: Finished instantiating SUT[llama2-70b-Server-SUT Rank1]
I0228 16:48:04.110981 21561 main_llm.cc:600] Rank 1:: Started follower SUT
I0228 16:48:04.111160 21561 main_llm.cc:606] Rank 1:: Test should start now.
I0228 16:48:04.111193 21560 main_llm.cc:606] Rank 0:: Test should start now.
I0228 16:48:04.111265 21562 main_llm.cc:544] Rank 2:: Init phase done on all the nodes.
I0228 16:48:04.111320 21562 main_llm.cc:546] Rank 2:: Starting running actual test.
================================================
MLPerf Results Summary
================================================
SUT name : llama2-70b-Server-SUT-LoadGen
Scenario : Server
Mode     : PerformanceOnly
Completed samples per second    : 5.61
Result is : INVALID
  Performance constraints satisfied : Yes
  Min duration satisfied : Yes
  Min queries satisfied : Yes
  Early stopping satisfied: NO
Recommendations:
TTFT Early Stopping Result:

TPOT Early Stopping Result:
 * Run unsuccessful.
 * Processed 100 queries.
 * Would need to run at least 359 more queries,
 with the run being successful if every additional
 query were under latency.

================================================
Additional Stats
================================================
Scheduled samples per second : 12.99

Min latency (ns)                : 428438331
Max latency (ns)                : 31500768954
Mean latency (ns)               : 11433009592
50.00 percentile latency (ns)   : 9700987002
90.00 percentile latency (ns)   : 22442637774
95.00 percentile latency (ns)   : 24454158586
97.00 percentile latency (ns)   : 26447565100
99.00 percentile latency (ns)   : 31500768954
99.90 percentile latency (ns)   : 31500768954

Completed tokens per second                 : 1622.67
Min First Token latency (ns)                : 80680009
Max First Token latency (ns)                : 876330066
Mean First Token latency (ns)               : 217360483
50.00 percentile first token latency (ns)   : 143833233
90.00 percentile first token latency (ns)   : 496639062
95.00 percentile first token latency (ns)   : 741176950
97.00 percentile first token latency (ns)   : 840376043
99.00 percentile first token latency (ns)   : 876330066
99.90 percentile first token latency (ns)   : 876330066

Min Time to Output Token (ns)                : 35253629
Max Time to Output Token (ns)                : 52609348
Mean Time to Output Token (ns)               : 41086991
50.00 percentile time to output token (ns)   : 40730642
90.00 percentile time to output token (ns)   : 47417231
95.00 percentile time to output token (ns)   : 48156453
97.00 percentile time to output token (ns)   : 48462602
99.00 percentile time to output token (ns)   : 52609348
99.90 percentile time to output token (ns)   : 52609348

================================================
Test Parameters Used
================================================
samples_per_query : 1
target_qps : 11.04
ttft_latency (ns): 2000000000
tpot_latency (ns): 200000000
max_async_queries : 0
min_duration (ms): 0
max_duration (ms): 0
min_query_count : 100
max_query_count : 0
qsl_rng_seed : 13281865557512327830
sample_index_rng_seed : 198141574272810017
schedule_rng_seed : 7575108116881280410
accuracy_log_rng_seed : 720381539243781796
accuracy_log_probability : 0
accuracy_log_sampling_target : 100
print_timestamps : 0
performance_issue_unique : 0
performance_issue_same : 0
performance_issue_same_index : 0
performance_sample_count : 24576

1 warning encountered. See detailed log.

No errors encountered during test.
I0228 16:48:36.897497 21562 main_llm.cc:548] Rank 2:: Finished running actual test.
I0228 16:48:36.897534 21562 main_llm.cc:550] Rank 2:: Initiating termination sequence.
I0228 16:48:36.897557 21562 main_llm.cc:552] Rank 2:: Termination sequence initiated, waiting for others...
I0228 16:48:36.897661 21585 llm_server.cc:1139] Rank 2:: llama2-70b-Server-SUT-LoadGen Port[0] unblocked from first-token receive for shut-down
I0228 16:48:36.897675 21583 llm_server.cc:1029] Rank 2:: llama2-70b-Server-SUT-LoadGen Port[0] unblocked from RECV for shut-down
I0228 16:48:36.897712 21583 llm_server.cc:953] Rank 2:: llama2-70b-Server-SUT-LoadGen Stopping...
I0228 16:48:36.897647 21593 llm_server.cc:584] Rank 0:: llama2-70b-Server-SUT Rank0 unblocked from RECV to shut-down
I0228 16:48:37.897766 21560 main_llm.cc:611] Rank 0:: Test should finish now.
I0228 16:48:37.897716 21562 main_llm.cc:557] Rank 2:: Sync'ed with others for shutting-off.
I0228 16:48:37.897739 21561 main_llm.cc:611] Rank 1:: Test should finish now.
I0228 16:48:37.916193 21562 main_llm.cc:618] Rank 2:: All done
[TensorRT-LLM][INFO] Terminate signal received, worker thread exiting.
[TensorRT-LLM][INFO] Terminate signal received, worker thread exiting.
I0228 16:48:38.479334 21560 main_llm.cc:618] Rank 0:: All done
I0228 16:48:38.495682 21561 main_llm.cc:618] Rank 1:: All done
[2024-02-28 16:48:40,184 run_harness.py:165 INFO] Result: result_completed_samples_per_sec: 5.61419, Result is INVALID
[2024-02-28 16:48:40,184 run_audit.py:162 INFO] => Running compliance harness verification script...
[2024-02-28 16:48:40,184 __init__.py:46 INFO] Running command: /usr/bin/python3.8 /work/build/inference/compliance/nvidia/TEST06/run_verification.py --compliance_dir=build/compliance_logs/TEST06/NC_H100_v5_TRT/llama2-70b-99/Server --output_dir=build/compliance_logs/TEST06/NC_H100_v5_TRT/llama2-70b-99/Server --scenario=Server --dtype=int32
First token check pass: True
EOS check pass: True
Sample length check pass: True
TEST06 verification complete

[2024-02-28 16:48:40,279 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:48:40,279 auditing.py:156 INFO] Audit cleanup: Removing file audit.config
[2024-02-28 16:48:40,279 run_audit.py:320 INFO] => Submission checker: Audit test TEST06 PASS
[2024-02-28 16:48:40,288 run_audit.py:142 INFO] => Running compliance harness for test TEST06
[2024-02-28 16:48:40,288 auditing.py:139 INFO] AUDIT HARNESS: Looking for audit.config in build/inference/compliance/nvidia/TEST06/llama2-70b/audit.config...
[2024-02-28 16:48:40,288 auditing.py:143 INFO] AUDIT HARNESS: Search failed. Looking for audit.config in build/inference/compliance/nvidia/TEST06/audit.config...
[2024-02-28 16:48:40,289 generate_conf_files.py:107 INFO] Generated measurements/ entries for NC_H100_v5_TRT/llama2-70b-99.9/Server
[2024-02-28 16:48:40,289 __init__.py:46 INFO] Running command: mpirun -np 3 ./build/bin/harness_llm --logfile_outdir="build/compliance_logs/TEST06/NC_H100_v5_TRT/llama2-70b-99.9/Server" --logfile_prefix="mlperf_log_" --performance_sample_count=24576 --gpu_batch_size=1536 --tensor_path="build/preprocessed_data/open_orca/input_ids_padded.npy,build/preprocessed_data/open_orca/input_lens.npy" --use_graphs=false --gpu_inference_streams=1 --gpu_copy_streams=1 --tensor_parallelism=2 --pipeline_parallelism=1 --kvcache_free_gpu_mem_frac=0.9 --enable_sort=false --llm_gen_config_path="code/llama2-70b/tensorrt/generation_config.json" --use_token_latencies=true --gpu_engines="./build/engines/NC_H100_v5/llama2-70b/Server/bs1536-custom_k_99_9_MaxP-tp2-pp1/rank0.engine" --mlperf_conf_path="build/loadgen-configs/NC_H100_v5_TRT/llama2-70b-99.9/Server/mlperf.conf" --user_conf_path="build/loadgen-configs/NC_H100_v5_TRT/llama2-70b-99.9/Server/user.conf" --scenario Server --model llama2-70b
[2024-02-28 16:48:40,289 __init__.py:53 INFO] Overriding Environment
benchmark : Benchmark.LLAMA2
buffer_manager_thread_count : 0
coalesced_tensor : True
data_dir : /mnt/resource_nvme/scratch/data
enable_sort : False
gpu_batch_size : 1536
gpu_copy_streams : 1
gpu_inference_streams : 1
input_dtype : int32
input_format : linear
kvcache_free_gpu_mem_frac : 0.9
llm_gen_config_path : code/llama2-70b/tensorrt/generation_config.json
log_dir : build/compliance_logs/TEST06
pipeline_parallelism : 1
precision : fp16
preprocessed_data_dir : /mnt/resource_nvme/scratch/preprocessed_data
scenario : Scenario.Server
server_target_qps : 12
server_target_qps_adj_factor : 0.92
system : SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='AMD EPYC 9V84 96-Core Processor', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=80, threads_per_core=1): 1}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=660.463936, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=660463936000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA H100 NVL', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=93.583984375, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=100485038080), max_power_limit=400.0, pci_id='0x232110DE', compute_sm=90): 2})), numa_conf=NUMAConfiguration(numa_nodes={}, num_numa_nodes=2), system_id='NC_H100_v5')
tensor_parallelism : 2
tensor_path : build/preprocessed_data/open_orca/input_ids_padded.npy,build/preprocessed_data/open_orca/input_lens.npy
use_fp8 : True
use_graphs : False
use_token_latencies : True
system_id : NC_H100_v5
config_name : NC_H100_v5_llama2-70b_Server
workload_setting : WorkloadSetting(HarnessType.Custom, AccuracyTarget.k_99_9, PowerSetting.MaxP)
optimization_level : plugin-enabled
num_profiles : 1
config_ver : custom_k_99_9_MaxP
accuracy_level : 99.9%
inference_server : custom
audit_test_name : TEST06
skip_file_checks : False
power_limit : None
cpu_freq : None
&&&& RUNNING LLM_HARNESS # ./build/bin/harness_llm
[I] Initializing TensorRT plugin library...
[I] Initializing TRT-LLM plugin libraries...
&&&& RUNNING LLM_HARNESS # ./build/bin/harness_llm
[I] Initializing TensorRT plugin library...
[I] Initializing TRT-LLM plugin libraries...
&&&& RUNNING LLM_HARNESS # ./build/bin/harness_llm
[I] Initializing TensorRT plugin library...
[I] Initializing TRT-LLM plugin libraries...
I0228 16:48:43.705379 21670 main_llm.cc:170] Loading LLM config from "./build/engines/NC_H100_v5/llama2-70b/Server/bs1536-custom_k_99_9_MaxP-tp2-pp1/config.json" and geneartion config from: code/llama2-70b/tensorrt/generation_config.json
I0228 16:48:43.710700 21669 main_llm.cc:170] Loading LLM config from "./build/engines/NC_H100_v5/llama2-70b/Server/bs1536-custom_k_99_9_MaxP-tp2-pp1/config.json" and geneartion config from: code/llama2-70b/tensorrt/generation_config.json
I0228 16:48:43.727661 21668 main_llm.cc:170] Loading LLM config from "./build/engines/NC_H100_v5/llama2-70b/Server/bs1536-custom_k_99_9_MaxP-tp2-pp1/config.json" and geneartion config from: code/llama2-70b/tensorrt/generation_config.json
I0228 16:48:43.945062 21670 main_llm.cc:284] Rank: 2 has pid: 21670
I0228 16:48:43.945092 21670 main_llm.cc:121] Found 2 GPUs
I0228 16:48:43.945042 21669 main_llm.cc:284] Rank: 1 has pid: 21669
I0228 16:48:43.945068 21669 main_llm.cc:121] Found 2 GPUs
I0228 16:48:43.945163 21668 main_llm.cc:281] LLMConfig Details:
	mMaxSumSeqlen: 2048 mMaxInputSeqlen: 1024 mMaxOutputSeqlen: 1024 mEosId: 2
	tp: 2 pp: 1 mWorldSize: 2
	mMaxGpuBatchSize: 1536 mBeamWidth: 1
	mTemperature: 1 mTopK: 1 mTopP: 0 mMinOutputSeqlen: 1 mIsStreaming: 1 mReportFirstToken: 1 mEnableSort: 0 mExcludeInputInOutput: 1
	mMaxNumSequences: 0 mMaxTokensInPagedKvcache: 0 mKvCacheFreeGpuMemFraction: 0.9 mEnableTrtOverlap: 0
	mBatchMode: 2 mSchedulerPolicy: 0
I0228 16:48:43.945192 21668 main_llm.cc:284] Rank: 0 has pid: 21668
I0228 16:48:43.945197 21668 main_llm.cc:121] Found 2 GPUs
I0228 16:48:43.945376 21668 main_llm.cc:506] Rank 0:: local session [rank/size | color | devId]: [0/2 | 0 | 0]
I0228 16:48:43.945384 21668 main_llm.cc:585] Rank 0:: Started instantiating SUT[llama2-70b-Server-SUT Rank0]
I0228 16:48:43.945369 21670 main_llm.cc:506] Rank 2:: local session [rank/size | color | devId]: [0/1 | 1 | -]
I0228 16:48:43.945381 21670 main_llm.cc:529] Rank 2:: Started instantiating QSL[llama2-70b-Server-QSL-LoadGen]
I0228 16:48:43.945370 21669 main_llm.cc:506] Rank 1:: local session [rank/size | color | devId]: [1/2 | 0 | 1]
I0228 16:48:43.945381 21669 main_llm.cc:585] Rank 1:: Started instantiating SUT[llama2-70b-Server-SUT Rank1]
I0228 16:48:43.945641 21668 llm_server.cc:406] LLMServer[llama2-70b-Server-SUT Rank0] creating LLMCore[0] on Device[0]...
I0228 16:48:43.945648 21668 llm_core.cc:68] Rank 0:: LLMCore[0] at Device Id [0]
[TensorRT-LLM][INFO] Engine version 0.9.0.dev2024020600 found in the config file, assuming engine(s) built by new builder API.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'lora_target_modules' not found
[TensorRT-LLM][WARNING] Optional value for parameter lora_target_modules will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'num_medusa_heads' not found
[TensorRT-LLM][WARNING] Optional value for parameter num_medusa_heads will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'max_draft_len' not found
[TensorRT-LLM][WARNING] Optional value for parameter max_draft_len will not be set.
[TensorRT-LLM][INFO] MPI size: 2, rank: 0
[TensorRT-LLM][INFO] Using user-specified devices: (0, 1)
[TensorRT-LLM][INFO] Rank 0 is using GPU 0
I0228 16:48:43.994123 21670 main_llm.cc:532] Rank 2:: Finished instantiating QSL[llama2-70b-Server-QSL-LoadGen]
I0228 16:48:43.994150 21670 main_llm.cc:534] Rank 2:: Started instantiating SUT[llama2-70b-Server-SUT-LoadGen]
I0228 16:48:44.005048 21670 main_llm.cc:539] Rank 2:: Finished instantiating SUT[llama2-70b-Server-SUT-LoadGen]
I0228 16:48:44.005062 21670 main_llm.cc:541] Rank 2:: Waiting for the end of init phase of other nodes...
I0228 16:48:44.580964 21669 llm_server.cc:406] LLMServer[llama2-70b-Server-SUT Rank1] creating LLMCore[0] on Device[1]...
I0228 16:48:44.581009 21669 llm_core.cc:68] Rank 1:: LLMCore[0] at Device Id [1]
[TensorRT-LLM][INFO] Engine version 0.9.0.dev2024020600 found in the config file, assuming engine(s) built by new builder API.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'lora_target_modules' not found
[TensorRT-LLM][WARNING] Optional value for parameter lora_target_modules will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'num_medusa_heads' not found
[TensorRT-LLM][WARNING] Optional value for parameter num_medusa_heads will not be set.
[TensorRT-LLM][WARNING] [json.exception.out_of_range.403] key 'max_draft_len' not found
[TensorRT-LLM][WARNING] Optional value for parameter max_draft_len will not be set.
[TensorRT-LLM][INFO] MPI size: 2, rank: 1
[TensorRT-LLM][INFO] Using user-specified devices: (0, 1)
[TensorRT-LLM][INFO] Rank 1 is using GPU 1
[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 1536
[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 1536
[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: 2048
[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0
[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1
[TensorRT-LLM][INFO] Loaded engine size: 33176 MiB
[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 1536
[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 1536
[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: 2048
[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0
[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1
[TensorRT-LLM][INFO] Loaded engine size: 33176 MiB
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +66, now: CPU 33257, GPU 35337 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +2, GPU +72, now: CPU 33259, GPU 35409 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +66, now: CPU 33273, GPU 33750 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +2, GPU +72, now: CPU 33275, GPU 33822 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +33142, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +33142, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] [MS] Running engine with multi stream info
[TensorRT-LLM][INFO] [MS] Number of aux streams is 1
[TensorRT-LLM][INFO] [MS] Number of total worker streams is 2
[TensorRT-LLM][INFO] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 33551, GPU 34914 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 33551, GPU 34978 (MiB)
[TensorRT-LLM][INFO] [MS] Running engine with multi stream info
[TensorRT-LLM][INFO] [MS] Number of aux streams is 1
[TensorRT-LLM][INFO] [MS] Number of total worker streams is 2
[TensorRT-LLM][INFO] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +64, now: CPU 33535, GPU 36501 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +0, GPU +64, now: CPU 33535, GPU 36565 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 33142 (MiB)
[TensorRT-LLM][INFO] Allocate 52617543680 bytes for k/v cache. 
[TensorRT-LLM][INFO] Using 642304 total tokens in paged KV cache, and 16 blocks per sequence
[TensorRT-LLM][INFO] Allocate 52617543680 bytes for k/v cache. 
[TensorRT-LLM][INFO] Using 642304 total tokens in paged KV cache, and 16 blocks per sequence
I0228 16:49:43.243721 21668 llm_core.cc:91] Rank 0:: LLMCore Setup complete for device 0
I0228 16:49:43.243805 21668 llm_server.cc:411] LLMServer[llama2-70b-Server-SUT Rank0] created LLMCore[0] on Device[0].
I0228 16:49:43.253134 21668 main_llm.cc:592] Rank 0:: Finished instantiating SUT[llama2-70b-Server-SUT Rank0]
I0228 16:49:43.253145 21668 main_llm.cc:593] Rank 0:: Started Leader SUT
I0228 16:49:43.544114 21669 llm_core.cc:91] Rank 1:: LLMCore Setup complete for device 1
I0228 16:49:43.544179 21669 llm_server.cc:411] LLMServer[llama2-70b-Server-SUT Rank1] created LLMCore[0] on Device[1].
I0228 16:49:43.544188 21669 main_llm.cc:599] Rank 1:: Finished instantiating SUT[llama2-70b-Server-SUT Rank1]
I0228 16:49:43.544189 21669 main_llm.cc:600] Rank 1:: Started follower SUT
I0228 16:49:43.544405 21669 main_llm.cc:606] Rank 1:: Test should start now.
I0228 16:49:43.544382 21668 main_llm.cc:606] Rank 0:: Test should start now.
I0228 16:49:43.544422 21670 main_llm.cc:544] Rank 2:: Init phase done on all the nodes.
I0228 16:49:43.544477 21670 main_llm.cc:546] Rank 2:: Starting running actual test.
================================================
MLPerf Results Summary
================================================
SUT name : llama2-70b-Server-SUT-LoadGen
Scenario : Server
Mode     : PerformanceOnly
Completed samples per second    : 5.61
Result is : INVALID
  Performance constraints satisfied : Yes
  Min duration satisfied : Yes
  Min queries satisfied : Yes
  Early stopping satisfied: NO
Recommendations:
TTFT Early Stopping Result:

TPOT Early Stopping Result:
 * Run unsuccessful.
 * Processed 100 queries.
 * Would need to run at least 359 more queries,
 with the run being successful if every additional
 query were under latency.

================================================
Additional Stats
================================================
Scheduled samples per second : 12.99

Min latency (ns)                : 432372100
Max latency (ns)                : 31623540692
Mean latency (ns)               : 11500399867
50.00 percentile latency (ns)   : 9737783581
90.00 percentile latency (ns)   : 22484784682
95.00 percentile latency (ns)   : 24563436967
97.00 percentile latency (ns)   : 26558261422
99.00 percentile latency (ns)   : 31623540692
99.90 percentile latency (ns)   : 31623540692

Completed tokens per second                 : 1621.44
Min First Token latency (ns)                : 80410078
Max First Token latency (ns)                : 875263406
Mean First Token latency (ns)               : 220635708
50.00 percentile first token latency (ns)   : 156600292
90.00 percentile first token latency (ns)   : 495568851
95.00 percentile first token latency (ns)   : 740106659
97.00 percentile first token latency (ns)   : 839311323
99.00 percentile first token latency (ns)   : 875263406
99.90 percentile first token latency (ns)   : 875263406

Min Time to Output Token (ns)                : 35316094
Max Time to Output Token (ns)                : 52822247
Mean Time to Output Token (ns)               : 41397097
50.00 percentile time to output token (ns)   : 41060149
90.00 percentile time to output token (ns)   : 47950179
95.00 percentile time to output token (ns)   : 49088428
97.00 percentile time to output token (ns)   : 49191735
99.00 percentile time to output token (ns)   : 52822247
99.90 percentile time to output token (ns)   : 52822247

================================================
Test Parameters Used
================================================
samples_per_query : 1
target_qps : 11.04
ttft_latency (ns): 2000000000
tpot_latency (ns): 200000000
max_async_queries : 0
min_duration (ms): 0
max_duration (ms): 0
min_query_count : 100
max_query_count : 0
qsl_rng_seed : 13281865557512327830
sample_index_rng_seed : 198141574272810017
schedule_rng_seed : 7575108116881280410
accuracy_log_rng_seed : 720381539243781796
accuracy_log_probability : 0
accuracy_log_sampling_target : 100
print_timestamps : 0
performance_issue_unique : 0
performance_issue_same : 0
performance_issue_same_index : 0
performance_sample_count : 24576

1 warning encountered. See detailed log.

No errors encountered during test.
I0228 16:50:16.448272 21670 main_llm.cc:548] Rank 2:: Finished running actual test.
I0228 16:50:16.448311 21670 main_llm.cc:550] Rank 2:: Initiating termination sequence.
I0228 16:50:16.448330 21670 main_llm.cc:552] Rank 2:: Termination sequence initiated, waiting for others...
I0228 16:50:16.448431 21701 llm_server.cc:584] Rank 0:: llama2-70b-Server-SUT Rank0 unblocked from RECV to shut-down
I0228 16:50:16.448431 21693 llm_server.cc:1139] Rank 2:: llama2-70b-Server-SUT-LoadGen Port[0] unblocked from first-token receive for shut-down
I0228 16:50:16.448477 21691 llm_server.cc:1029] Rank 2:: llama2-70b-Server-SUT-LoadGen Port[0] unblocked from RECV for shut-down
I0228 16:50:16.448509 21691 llm_server.cc:953] Rank 2:: llama2-70b-Server-SUT-LoadGen Stopping...
I0228 16:50:17.448482 21669 main_llm.cc:611] Rank 1:: Test should finish now.
I0228 16:50:17.448482 21670 main_llm.cc:557] Rank 2:: Sync'ed with others for shutting-off.
I0228 16:50:17.448539 21668 main_llm.cc:611] Rank 0:: Test should finish now.
I0228 16:50:17.467191 21670 main_llm.cc:618] Rank 2:: All done
[TensorRT-LLM][INFO] Terminate signal received, worker thread exiting.
[TensorRT-LLM][INFO] Terminate signal received, worker thread exiting.
I0228 16:50:18.031327 21668 main_llm.cc:618] Rank 0:: All done
I0228 16:50:18.046208 21669 main_llm.cc:618] Rank 1:: All done
[2024-02-28 16:50:19,737 run_harness.py:165 INFO] Result: result_completed_samples_per_sec: 5.60994, Result is INVALID
[2024-02-28 16:50:19,738 run_audit.py:162 INFO] => Running compliance harness verification script...
[2024-02-28 16:50:19,738 __init__.py:46 INFO] Running command: /usr/bin/python3.8 /work/build/inference/compliance/nvidia/TEST06/run_verification.py --compliance_dir=build/compliance_logs/TEST06/NC_H100_v5_TRT/llama2-70b-99.9/Server --output_dir=build/compliance_logs/TEST06/NC_H100_v5_TRT/llama2-70b-99.9/Server --scenario=Server --dtype=int32
First token check pass: True
EOS check pass: True
Sample length check pass: True
TEST06 verification complete

[2024-02-28 16:50:19,834 run_audit.py:311 INFO] => Audit harness: Cleaning up audit.config...
[2024-02-28 16:50:19,834 auditing.py:156 INFO] Audit cleanup: Removing file audit.config
[2024-02-28 16:50:19,834 run_audit.py:320 INFO] => Submission checker: Audit test TEST06 PASS
make[1]: Leaving directory '/work'
